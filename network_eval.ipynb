{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "NETWORK_FILE_PATH = './cs109_final_backend/cs109_final_backend/network_files/pickled_network.pickle'\n",
    "\n",
    "with open(NETWORK_FILE_PATH, 'rb') as f: \n",
    "    NETWORK = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that will give n predictions for a given list of songs\n",
    "\n",
    "This function takes a list of song uris (ie the current playlist) and then returns n=num_top_songs predicted songs in decreasing order of relevance. From each song, it takes num_samples 1-step weighted random walks from the song-node. The samples are then summed and normalized to determine which ones are most important (the ones that are found the most are deemed to be the most important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_top_songs(playlist_songs, network, num_samples=4000, num_top_songs=100): \n",
    "    \"\"\" Get the n-top song predictions for a given playlist by using the network\n",
    "    \n",
    "    Args: \n",
    "        playlist_songs: (uri str list) List of spotify track uris; the current playlist\n",
    "        network : (dict) The network that we build using build_network.py\n",
    "        num_samples: (int) The number of samples to take from each song in the current playlist\n",
    "        num_top_songs: (int)\n",
    "        \n",
    "    Returns: \n",
    "        counted_samples : ((uri (str), prob (float)) tuple list) The n top samples with respective probabilities \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    key_errors = 0\n",
    "    all_samples = np.array([])\n",
    "    for song_uri in playlist_songs: \n",
    "        try : \n",
    "            sample = np.random.choice(network[song_uri]['songs'], num_samples, p=network[song_uri]['counts'])\n",
    "            all_samples = np.append(all_samples, sample)\n",
    "        except KeyError: \n",
    "            key_errors += 1\n",
    "    \n",
    "    unique, counts = np.unique(all_samples, return_counts=True)\n",
    "    \n",
    "    counts = counts.astype(float) / np.sum(counts)\n",
    "    counted_samples = zip(unique, counts)\n",
    "    counted_samples = [sample for sample in counted_samples if sample[0] not in playlist_songs]\n",
    "    counted_samples = sorted(counted_samples, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    num_to_return = min(num_top_songs, len(counted_samples))\n",
    "\n",
    "    return counted_samples[:num_to_return]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to build the evaluation datasets \n",
    "\n",
    "This function reads through playlists that the model has not been trained on, and creates a list of predictor lists (the first n songs in a playlist) and a list of remaining songs (the last total - n songs in the playlist). This is then used to check how many of the remaining songs are in the 500 predicted songs made from the first n songs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_evaluation_dataset(start, blocks = 1, n_predictors=10, min_remaining = 100, max_remaining = 125) : \n",
    "    \"\"\" Build a list of first n song lists, and a list of last total - n song lists \n",
    "    \n",
    "    Args: \n",
    "        start : (int) the starting playlist slice\n",
    "        blocks : (int) The number of playlist slices to use\n",
    "        n_predictor : (int) The number of songs to be in the list of predictor lists\n",
    "        min_remaining : (int) The minimum number of songs remaining on the playlist\n",
    "        max_remaining : (int) The maximum number of songs remaining on the playlist\n",
    "        \n",
    "    Returns: \n",
    "        predictor_songs : ((str list) list) List of predictor song lists\n",
    "        remainder_songs : ((str list) list) List of remaining songs (the ones we're trying to guess)\n",
    "    \n",
    "    \"\"\"\n",
    "    f_start = start * 1000\n",
    "    f_end = start * 1000 + 999\n",
    "    predictor_songs = []\n",
    "    remainder_songs = []\n",
    "    for i in range(blocks): \n",
    "        with open('./mpd.v1/data/mpd.slice.{}-{}.json'.format(f_start, f_end)) as f :\n",
    "            data = json.load(f)\n",
    "            \n",
    "            for playlist in data['playlists'] : \n",
    "                tracks = [t['track_uri'] for t in playlist['tracks']]\n",
    "                if len(tracks) >= min_remaining + n_predictors and len(tracks) <= max_remaining + n_predictors: \n",
    "                    predict = tracks[:n_predictors]\n",
    "                    remain = tracks[n_predictors:]\n",
    "\n",
    "                    predictor_songs.append(predict)\n",
    "                    remainder_songs.append(remain)\n",
    "            \n",
    "            \n",
    "    return predictor_songs, remainder_songs            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to evaluate the accuracy of the predictions\n",
    "\n",
    "This function takes the first n predictor songs, gets the top predictions, and then evaluates what percent of the remaining songs are in the predicted songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(predictors, remaining, network, num_predictions, num_samples=4000): \n",
    "    \"\"\" Returns the percent of remaining songs that are found in the predictions from the \n",
    "        network model \n",
    "        \n",
    "    Args: \n",
    "        predictors : ((str list) list) List of predictor song lists\n",
    "        remaining : ((str list) list) List of the remaining song lists\n",
    "        network : (dict) The network/model \n",
    "        num_predictions : (int) The number of songs that the model should predict\n",
    "        num_samples : (int) The number of samples to take from each song\n",
    "        \n",
    "    Returns: \n",
    "        accuracy: (float) Ratio of remaining songs in the predictions over the\n",
    "                total number of remaining songs\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    preds = n_top_songs(predictors, network, num_top_songs = num_predictions, num_samples=num_samples)\n",
    "    preds = [p[0] for p in preds]\n",
    "    accuracy = len([x for x in preds if x in remaining])/(1. * len(remaining))\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Now it's time to start evaluating the model using our above evaluation method. We will stick with predicting songs for playlists with between 100-125 remaining songs (S)so that the number of remaining songs does not impact the accuarcy of the predictions. We will then do this for the number of predictor songs (k) = [1,5,10,25,100]. First we will do this building test sets from the same set of slices, and then we will build distributions of the mean accuracies/medains from different test slices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 1, 100 <= S <= 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the predictors / remainders lists \n",
    "predictors, remainders = build_evaluation_dataset(100, blocks=15,n_predictors=1)\n",
    "\n",
    "# Print out the number of test points we've created\n",
    "print(\"We made {} test data points\".format(len(predictors)))\n",
    "\n",
    "# get the accuracies\n",
    "accuracies = []\n",
    "for pred, rem in zip(predictors, remainders): \n",
    "    accuracies.append(evaluate_accuracy(pred, rem, NETWORK, 500))\n",
    "\n",
    "# print the stats\n",
    "print(\"Mean accuracy: {}\".format(np.mean(accuracies)))\n",
    "print(\"Variance of accuracy: {}\".format(np.var(accuracies)))\n",
    "print(\"Median accuracy: {}\".format(np.median(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot a histogram of the accuracies\n",
    "bins = np.linspace(0,1,10)\n",
    "plt.hist(accuracies, bins=bins)\n",
    "plt.title('Accuracy Histogram. K=1')\n",
    "plt.xlabel('Remaining Songs In 500 Predicted Songs / Total Remaining Songs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 5, 100 <= S <= 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the predictors / remainders lists \n",
    "predictors, remainders = build_evaluation_dataset(100, blocks=15,n_predictors=5)\n",
    "\n",
    "# Print out the number of test points we've created\n",
    "print(\"We made {} test data points\".format(len(predictors)))\n",
    "\n",
    "# get the accuracies\n",
    "accuracies = []\n",
    "for pred, rem in zip(predictors, remainders): \n",
    "    accuracies.append(evaluate_accuracy(pred, rem, NETWORK, 500))\n",
    "\n",
    "# print the stats\n",
    "print(\"Mean accuracy: {}\".format(np.mean(accuracies)))\n",
    "print(\"Variance of accuracy: {}\".format(np.var(accuracies)))\n",
    "print(\"Median accuracy: {}\".format(np.median(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot a histogram of the accuracies\n",
    "bins = np.linspace(0,1,10)\n",
    "plt.hist(accuracies, bins=bins)\n",
    "plt.title('Accuracy Histogram. K=5')\n",
    "plt.xlabel('Remaining Songs In 500 Predicted Songs / Total Remaining Songs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 10, 100 <= S <= 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the predictors / remainders lists \n",
    "predictors, remainders = build_evaluation_dataset(100, blocks=15,n_predictors=10)\n",
    "\n",
    "# Print out the number of test points we've created\n",
    "print(\"We made {} test data points\".format(len(predictors)))\n",
    "\n",
    "# get the accuracies\n",
    "accuracies = []\n",
    "for pred, rem in zip(predictors, remainders): \n",
    "    accuracies.append(evaluate_accuracy(pred, rem, NETWORK, 500))\n",
    "\n",
    "# print the stats\n",
    "print(\"Mean accuracy: {}\".format(np.mean(accuracies)))\n",
    "print(\"Variance of accuracy: {}\".format(np.var(accuracies)))\n",
    "print(\"Median accuracy: {}\".format(np.median(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot a histogram of the accuracies\n",
    "bins = np.linspace(0,1,10)\n",
    "plt.hist(accuracies, bins=bins)\n",
    "plt.title('Accuracy Histogram. K=10')\n",
    "plt.xlabel('Remaining Songs In 500 Predicted Songs / Total Remaining Songs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 25, 100 <= S <= 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the predictors / remainders lists \n",
    "predictors, remainders = build_evaluation_dataset(100, blocks=20,n_predictors=25)\n",
    "\n",
    "# Print out the number of test points we've created\n",
    "print(\"We made {} test data points\".format(len(predictors)))\n",
    "\n",
    "# get the accuracies\n",
    "accuracies = []\n",
    "for pred, rem in zip(predictors, remainders): \n",
    "    accuracies.append(evaluate_accuracy(pred, rem, NETWORK, 500))\n",
    "\n",
    "# print the stats\n",
    "print(\"Mean accuracy: {}\".format(np.mean(accuracies)))\n",
    "print(\"Variance of accuracy: {}\".format(np.var(accuracies)))\n",
    "print(\"Median accuracy: {}\".format(np.median(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot a histogram of the accuracies\n",
    "bins = np.linspace(0,1,10)\n",
    "plt.hist(accuracies, bins=bins)\n",
    "plt.title('Accuracy Histogram. K=25')\n",
    "plt.xlabel('Remaining Songs In 500 Predicted Songs / Total Remaining Songs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 100, 100 <= S <= 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the predictors / remainders lists \n",
    "predictors, remainders = build_evaluation_dataset(100, blocks=30,n_predictors=100)\n",
    "\n",
    "# Print out the number of test points we've created\n",
    "print(\"We made {} test data points\".format(len(predictors)))\n",
    "\n",
    "# get the accuracies\n",
    "accuracies = []\n",
    "for pred, rem in zip(predictors, remainders): \n",
    "    accuracies.append(evaluate_accuracy(pred, rem, NETWORK, 500, num_samples=800))\n",
    "\n",
    "# print the stats\n",
    "print(\"Mean accuracy: {}\".format(np.mean(accuracies)))\n",
    "print(\"Variance of accuracy: {}\".format(np.var(accuracies)))\n",
    "print(\"Median accuracy: {}\".format(np.median(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot a histogram of the accuracies\n",
    "bins = np.linspace(0,1,10)\n",
    "plt.hist(accuracies, bins=bins)\n",
    "plt.title('Accuracy Histogram. K=100')\n",
    "plt.xlabel('Remaining Songs In 500 Predicted Songs / Total Remaining Songs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean distributions over different evaluation slices. k = [1,5,10]\n",
    "\n",
    "Here we're going to take a look at the distribution of the evaluation means from different test slices (just like above, but building a bunch of different test sets). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K = 1, 100 <= S <= 125, 80 random slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = []\n",
    "remainders = []\n",
    "\n",
    "# randomly take 80 slices\n",
    "for s in np.random.randint(100,1000, 80) :  \n",
    "    p, r = build_evaluation_dataset(s, blocks=1,n_predictors=1)\n",
    "    predictors.append(p)\n",
    "    remainders.append(r)\n",
    "\n",
    "\n",
    "# get the accuracies\n",
    "mean_accuracies = []\n",
    "med_accuracies = []\n",
    "for preds, rems in zip(predictors, remainders) : \n",
    "    accuracies = []\n",
    "    for p, r in zip(preds, rems): \n",
    "        accuracies.append(evaluate_accuracy(p, r, NETWORK, 500))\n",
    "    \n",
    "    mean_accuracies.append(np.mean(accuracies))\n",
    "    med_accuracies.append(np.median(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot a histogram of the mean accuracies and of the median accuracies\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "ax1.hist(mean_accuracies, bins=np.linspace(0,0.5,30))\n",
    "ax1.set_title('Distribution of Test-Slice Mean Accuracies, K=1')\n",
    "ax1.set_xlabel('Accuracy')\n",
    "\n",
    "ax2.hist(med_accuracies, bins=np.linspace(0,0.5,30))\n",
    "ax2.set_title('Distribution of Test-Slice Median Accuracies, K=1')\n",
    "ax2.set_xlabel('Accuracy')\n",
    "\n",
    "sns.kdeplot(med_accuracies, label='Median Accuracies', ax=ax3)\n",
    "sns.kdeplot(mean_accuracies, label='Mean Accuracies', ax=ax3)\n",
    "ax3.set_title('KDE Plot of Mean/Median Accuracies')\n",
    "ax3.set_xlabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K = 5, 100 <= S <= 125, 80 random slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = []\n",
    "remainders = []\n",
    "\n",
    "# randomly take 80 slices\n",
    "for s in np.random.randint(100,1000, 80) :  \n",
    "    p, r = build_evaluation_dataset(s, blocks=1,n_predictors=5)\n",
    "    predictors.append(p)\n",
    "    remainders.append(r)\n",
    "\n",
    "\n",
    "# get the accuracies\n",
    "mean_accuracies = []\n",
    "med_accuracies = []\n",
    "for preds, rems in zip(predictors, remainders) : \n",
    "    accuracies = []\n",
    "    for p, r in zip(preds, rems): \n",
    "        accuracies.append(evaluate_accuracy(p, r, NETWORK, 500))\n",
    "    \n",
    "    mean_accuracies.append(np.mean(accuracies))\n",
    "    med_accuracies.append(np.median(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot a histogram of the mean accuracies and of the median accuracies\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "ax1.hist(mean_accuracies, bins=np.linspace(0,0.5,30))\n",
    "ax1.set_title('Distribution of Test-Slice Mean Accuracies, K=5')\n",
    "ax1.set_xlabel('Accuracy')\n",
    "\n",
    "ax2.hist(med_accuracies, bins=np.linspace(0,0.5,30))\n",
    "ax2.set_title('Distribution of Test-Slice Median Accuracies, K=5')\n",
    "ax2.set_xlabel('Accuracy')\n",
    "\n",
    "sns.kdeplot(med_accuracies, label='Median Accuracies', ax=ax3)\n",
    "sns.kdeplot(mean_accuracies, label='Mean Accuracies', ax=ax3)\n",
    "ax3.set_title('KDE Plot of Mean/Median Accuracies')\n",
    "ax3.set_xlabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = []\n",
    "remainders = []\n",
    "\n",
    "# randomly take 80 slices\n",
    "for s in np.random.randint(100,1000, 80) :  \n",
    "    p, r = build_evaluation_dataset(s, blocks=1,n_predictors=10)\n",
    "    predictors.append(p)\n",
    "    remainders.append(r)\n",
    "\n",
    "\n",
    "# get the accuracies\n",
    "mean_accuracies = []\n",
    "med_accuracies = []\n",
    "for preds, rems in zip(predictors, remainders) : \n",
    "    accuracies = []\n",
    "    for p, r in zip(preds, rems): \n",
    "        accuracies.append(evaluate_accuracy(p, r, NETWORK, 500))\n",
    "    \n",
    "    mean_accuracies.append(np.mean(accuracies))\n",
    "    med_accuracies.append(np.median(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot a histogram of the mean accuracies and of the median accuracies\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "ax1.hist(mean_accuracies, bins=np.linspace(0,0.5,30))\n",
    "ax1.set_title('Distribution of Test-Slice Mean Accuracies, K=10')\n",
    "ax1.set_xlabel('Accuracy')\n",
    "\n",
    "ax2.hist(med_accuracies, bins=np.linspace(0,0.5,30))\n",
    "ax2.set_title('Distribution of Test-Slice Median Accuracies, K=10')\n",
    "ax2.set_xlabel('Accuracy')\n",
    "\n",
    "sns.kdeplot(med_accuracies, label='Median Accuracies', ax=ax3)\n",
    "sns.kdeplot(mean_accuracies, label='Mean Accuracies', ax=ax3)\n",
    "ax3.set_title('KDE Plot of Mean/Median Accuracies')\n",
    "ax3.set_xlabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "109_env",
   "language": "python",
   "name": "109_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
