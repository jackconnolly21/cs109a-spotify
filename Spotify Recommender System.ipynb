{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS109a Final Project - Spotify Recommendation System\n",
    "Group \\#55: Nick Kochanek, Jack Connolly, Chris Jarrett, Andrew Soldini\n",
    "\n",
    "**Project Goal:**  \n",
    "Our goal for this project was to develop a system that could recommend reasonable songs to continue a playlist give a certain number of \"seed\" tracks. We formalized this task as giving a model a list of $K$ input songs (as Spotify uris) and having the model output $500$ suggested uris (ideally ranked by relevance). This falls in line with the formal specifications for the Spotify RecSys challenge, and allows us to compare results and approaches with top teams there. As such, we decided to evaluate our models using the same metrics the contest was based on, which will be described further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and EDA\n",
    "The data sources we utilized in our explorations were the Million Playlist Dataset and the Spotify API. The MPD gave us the essential data to train models on subsets of playlists and evaluate the accuracy of our predictions. We used to Spotify API to get pre-computed audio features for songs, which allowed us to expore alternative model choices more in line with what we looked at in class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some EDA code, generate some cool graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More graphs and charts, both for MPD and for Spotify audio features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We explored a variety of different models and model types, but because the style of problem was different than those we studied in class, we were forced to explore different techniques than those we had seen. Our baseline models use a combination of KMeans Clustering and/or KNN, while the top two performing models are a Markov Walk on a Network and Collaborative Filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Song Based KMeans Clustering and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Song-Based KMeans code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another baseline: Playlist Based KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Playlist Based KNN (Chris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filtering code here (Chris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Based Markov Model\n",
    "This model is a probabilistic one that builds up a network where each vertex represents a song and each edge represents two song sharing a playlist (where more shared playlists leads to higher weighting). Then for prediction, for each of the input $K$ songs, many one step random walks are taken, and the most popular songs that show up in these walks are then returned as a list of 500 song recommendations (after getting rid of duplicates that are already in the playlist). This model consistently performed the best of our models, although building up a large network takes a significant amount of time and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Building code (Nick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "We decided to evaluate our models based on the same metrics used in the Spotify RecSys [contest rules](https://recsys-challenge.spotify.com/rules), namely R-Precision (RPrec), Normalized Discounted Cumulative Gain (NDCG), and Recommended Song Clicks (RSC). In the following definitions, $G$ is the set of ground truth tracks representing the held out songs from each playlist and $R$ is the ordered list of recommended songs returned by the recommendation system.\n",
    "\n",
    "* R-Precision: The metric counts \"number of retrieved relevant tracks divided by the number of known relevant tracks,\" rewarding the total number of retrieved relevant tracks, regardless of order.\n",
    "$$\\text{R-precision} = \\frac{\\left| G \\cap R_{1:|G|} \\right|}{|G|}$$\n",
    "\n",
    "* Normalized Discounted Cumulative Gain (NDCG): This metric takes into account the order of the returned songs, rewarding relevant songs placed higher in the returned list. It is calculated as Discounted Cumulative Gain (DCG), divided by the Ideal Discounted Cumulative Gain (IDCG), where the returned songs are ordered perfectly. That calculation looks like:\n",
    "$$DCG = rel_1 + \\sum_{i=2}^{|R|} \\frac{rel_i}{\\log_2 (i + 1)}$$\n",
    "$$IDCG = 1 + \\sum_{i=2}^{|G|} \\frac{1}{\\log_2 (i + 1)}$$\n",
    "$$NDCG = \\frac{DCG}{IDCG}$$\n",
    "\n",
    "* Recommended Songs Clicks (RSC): This measures how many \"clicks\" a Spotify user would need to find the first relevant song in the recommendations (the first song actually in the rest of the playlist $G$), where Spotify displays recommended songs in groups of 10. Therefore it's simply finding the first relevant song and returning its position in the list divided by 10 and truncated. Or more formally:\n",
    "$$\\text{clicks} = \\left\\lfloor \\frac{ \\arg\\min_i \\{ R_i\\colon R_i \\in G|\\} - 1}{10} \\right\\rfloor$$\n",
    "\n",
    "We have implemented these metrics in code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "class Evaluator():\n",
    "    \"\"\"Superclass for evaluation functions\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def evaluate(self, output, expected):\n",
    "        \"\"\"\n",
    "        Output will be the output of the model for some list of playlists\n",
    "        - Shape of (# playlists, 500)\n",
    "\n",
    "        Expected will be the held out songs from each playlist\n",
    "        - List of lists of various sizes\n",
    "\n",
    "        Note: Each \"song\" will be the unique spotify uri of a song\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class RPrecision(Evaluator):\n",
    "    \"\"\"\n",
    "    R-precision measures the number of held out songs correctly \n",
    "        retrieved by the model output \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Evaluator.__init__(self, 'R-Precision')\n",
    "        \n",
    "    def evaluate(self, output, expected, return_all=False):\n",
    "    \n",
    "        def rprec_one(output_, expected_):\n",
    "            expected_size = len(expected_)\n",
    "            common_set = set(output_).intersection(set(expected_))\n",
    "            common_size = len(common_set)\n",
    "            if expected_size == 0 or common_size == 0:\n",
    "                return 0.0\n",
    "            return common_size / expected_size\n",
    "        \n",
    "        r_precs = [rprec_one(out, exp) for (out, exp) in zip(output, expected)]\n",
    "        if return_all:\n",
    "            return np.mean(r_precs), r_precs\n",
    "        return np.mean(r_precs)\n",
    "\n",
    "    \n",
    "class NDCG(Evaluator):\n",
    "    \"\"\"\n",
    "    Normalized discounted cumulative gain also takes into \n",
    "        account how the system ordered the suggestions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Evaluator.__init__(self, 'NDCG')\n",
    "        \n",
    "    def evaluate(self, output, expected):\n",
    "        \n",
    "        def ndcg_one(output_, expected_):\n",
    "            dcg, idcg = 0.0, 0.0\n",
    "            \n",
    "            if len(output_) == 0 or len(expected_) == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            expected_ = set(expected_)\n",
    "            for i in range(len(output_)):\n",
    "                # Prediction DCG\n",
    "                if output_[i] in expected_:\n",
    "                    if i == 0:\n",
    "                        dcg += 1.0\n",
    "                    else:\n",
    "                        dcg += 1.0 / log2(i + 2.0)\n",
    "\n",
    "                if i < len(expected_):\n",
    "                    if i == 0:\n",
    "                        idcg += 1.0\n",
    "                    else:\n",
    "                        idcg += 1.0 / log2(i + 2.0)\n",
    "            \n",
    "            return dcg / idcg\n",
    "        \n",
    "        return np.mean([ndcg_one(out, exp) for (out, exp) in zip(output, expected)])\n",
    "        \n",
    "        \n",
    "class RSC(Evaluator):\n",
    "    \"\"\"\n",
    "    Recommended Song Clicks measures how many times a user\n",
    "    would have to click through the suggestions to find a song that \n",
    "    was a ground truth song\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Evaluator.__init__(self, 'RSC')\n",
    "        \n",
    "    def evaluate(self, output, expected):\n",
    "        \n",
    "        def rsc_one(output_, expected_):\n",
    "            if len(output_) == 0 or len(expected_) == 0:\n",
    "                return 51\n",
    "            \n",
    "            output_len = len(output_)\n",
    "            expected_ = set(expected_)\n",
    "            for i in range(output_len):\n",
    "                if output_[i] in expected_:\n",
    "                    return i//10\n",
    "            return 51\n",
    "        \n",
    "        return np.mean([rsc_one(out, exp) for (out, exp) in zip(output, expected)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(output, expected):\n",
    "    r_prec = RPrecision().evaluate(output, expected)\n",
    "    ndcg = NDCG().evaluate(output, expected)\n",
    "    rsc = RSC().evaluate(output, expected)\n",
    "    print(f\"R-Precision: {r_prec}, NCDG: {ndcg}, RSC: {rsc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation of each model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "#     output = model.predict(X_test)\n",
    "#     evaluate_model(output, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and Interpretations\n",
    "Overall, we have seen that the filtering and network models perform the best, significantly improving over the baseline models using nearest neighbor techniques. Our final models were comparable with some of the top models in the RecSys challenge, so we are very satisfied with our results. If we had more time and computing power, we would have liked to scale both of those models up larger, as they were both limited in terms of their size (the network was trained on about 14000 playlists and ended up being about 7GB while filtering was only able to handle about **HOW MANY PLAYLISTS**). Ideally, we would be able to utilize sklearn's suppoer for sparse matrices to scale up filtering, but we weren't able to finalize that. \n",
    "\n",
    "Music recommendation in general is a challenging problem, with millions of songs to choose from and a large variety of songs within. More complex techniques like deep RNNs and autoencoders seemed attractive at the beginning of the project, but ultimately weren't feasible for us to complete. This forced us to adapt and implement the fairly different models seen here. Overall, we feel confident in our model's ability to find relevant songs to continue and put together a great playlist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS 109",
   "language": "python",
   "name": "cs109"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
