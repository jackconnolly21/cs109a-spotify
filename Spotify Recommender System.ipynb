{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS109a Final Project - Spotify Recommendation System\n",
    "Group \\#55: Nick Kochanek, Jack Connolly, Chris Jarrett, Andrew Soldini\n",
    "\n",
    "**Project Goal:**  \n",
    "Our goal for this project was to develop a system that could recommend reasonable songs to continue a playlist give a certain number of \"seed\" tracks. We formalized this task as giving a model a list of $K$ input songs (as Spotify uris) and having the model output $500$ suggested uris (ideally ranked by relevance). This falls in line with the formal specifications for the Spotify RecSys challenge, and allows us to compare results and approaches with top teams there. As such, we decided to evaluate our models using the same metrics the contest was based on, which will be described further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "import pickle\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the data (Million Playlist Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After running into issues trying to pickle large objects (ie our graph), \n",
    "# It turns out that there's an issue in the pickle implementation. This stack overflow function \n",
    "# allows for easy saving of big objects \n",
    "# https://stackoverflow.com/questions/42653386/does-pickle-randomly-fail-with-oserror-on-large-files\n",
    "def save_as_pickled_object(obj, filepath):\n",
    "    \"\"\"\n",
    "    This is a defensive way to write pickle.write, allowing for very large files on all platforms\n",
    "    \"\"\"\n",
    "    max_bytes = 2**31 - 1\n",
    "    bytes_out = pickle.dumps(obj)\n",
    "    n_bytes = sys.getsizeof(bytes_out)\n",
    "    with open(filepath, 'wb') as f_out:\n",
    "        for idx in range(0, n_bytes, max_bytes):\n",
    "            f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the code from 'build_network.py'\n",
    "\n",
    "# 20 is a lot (the biggest we were able to run) \n",
    "# Use ~10 to finish in a reasonable time\n",
    "NUMBER_OF_FILES_TO_USE = 5\n",
    "\n",
    "\"\"\"\n",
    "The code below builds the network\n",
    "It also builds relevant objects to use for the other models\n",
    "\"\"\"\n",
    "song_name_to_uri, uri_to_song_name = {}, {}\n",
    "track_to_artist_album, network = {}, {}\n",
    "\n",
    "track_counts, artist_counts = {}, {}\n",
    "playlist_lens, artists_perplay = [], []\n",
    "\n",
    "\n",
    "track_codes = set()\n",
    "playlists, uri_input, uri_expected = [], [], []\n",
    "\n",
    "K = 10\n",
    "\n",
    "f_start = 1000\n",
    "f_end = 1999\n",
    "for i in range(NUMBER_OF_FILES_TO_USE): \n",
    "    with open('./mpd.v1/data/mpd.slice.{}-{}.json'.format(f_start, f_end)) as f: \n",
    "        data = json.load(f)\n",
    "        \n",
    "    input_, expected = [], []\n",
    "    for playlist in data['playlists']:\n",
    "        playlist_count = 0\n",
    "        play_artists = {}\n",
    "        playlist_dict = playlist.copy()\n",
    "        playlist_dict.pop('tracks', None)\n",
    "\n",
    "        for k, song in enumerate(playlist['tracks']): \n",
    "            track_name  = song['track_name']\n",
    "            track_uri = song['track_uri']\n",
    "            #shared_songs = np.array([s['track_uri'] for s in playlist['tracks'] if s['track_uri'] != track_uri])\n",
    "            playlist_count += 1\n",
    "            artist_name = song['artist_name']\n",
    "            \n",
    "            if track_uri not in track_codes:\n",
    "                track_codes.add(track_uri)\n",
    "                track_to_artist_album[track_uri] = {'artist': song['artist_name'], 'album': song['album_name']}\n",
    "                uri_to_song_name[track_uri] = track_name\n",
    "            \n",
    "            if track_name not in song_name_to_uri: \n",
    "                song_name_to_uri[track_name] = track_uri\n",
    "\n",
    "#             if track_uri not in network: \n",
    "#                 network[track_uri] = np.array(shared_songs)\n",
    "#             else: \n",
    "#                 network[track_uri] = np.append(network[track_uri], np.array(shared_songs))\n",
    "                \n",
    "            if k < K:\n",
    "                input_.append(track_uri)\n",
    "            else:\n",
    "                expected.append(track_uri)\n",
    "            \n",
    "            # EDA Stats Collecting\n",
    "            if artist_name not in artist_counts:\n",
    "                artist_counts[artist_name] = 1\n",
    "            else:\n",
    "                artist_counts[artist_name] += 1\n",
    "            \n",
    "            if artist_name not in play_artists:\n",
    "                play_artists[artist_name] = 1\n",
    "            else:\n",
    "                play_artists[artist_name] += 1\n",
    "                \n",
    "        playlists.append(playlist_dict)\n",
    "        uri_input.append(input_)\n",
    "        uri_expected.append(expected)\n",
    "        \n",
    "        # For EDA\n",
    "        playlist_lens.append(playlist_count)\n",
    "        artists_perplay.append(len(play_artists.keys()))\n",
    "                \n",
    "    print (\"done loading file\", i)             \n",
    "    f_start += 1000\n",
    "    f_end += 1000\n",
    "    \n",
    "    \n",
    "# Clean the network -> counts per song (normalized)\n",
    "# print(\"Cleaning up the Network a bit\")\n",
    "# for uri in network : \n",
    "#     unique, counts = np.unique(network[uri], return_counts=True)\n",
    "#     network[uri] = {'songs' : unique, 'counts': counts / np.sum(counts)}\n",
    "    \n",
    "    \n",
    "# # Save all of the objects as pickles\n",
    "# save_as_pickled_object(network, 'pickled_network.pickle')\n",
    "\n",
    "# with open('songs_to_uri.pickle', 'wb') as f:\n",
    "#     pickle.dump(song_name_to_uri, f)\n",
    "\n",
    "# with open('uri_to_song.pickle', 'wb') as f:\n",
    "#     pickle.dump(uri_to_song_name, f)\n",
    "\n",
    "# with open('track_to_artist_album.pickle', 'wb') as f:\n",
    "#     pickle.dump(track_to_artist_album, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting audio features from the Spotify API**\n",
    "This takes awhile - you can just load to saved pickles 'audio_features.pickle' and 'uris_10.pickle' below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the pickles instead of rescraping\n",
    "with open('./cs109_final_backend/cs109_final_backend/cluster_files/uris_10.pickle', 'rb') as f:    \n",
    "    uris = pickle.load(f)\n",
    "with open('./cs109_final_backend/cs109_final_backend/cluster_files/audio_features.pickle', 'rb') as f:    \n",
    "    audio_features = pickle.load(f)\n",
    "        \n",
    "audio_df = pd.DataFrame(audio_features)\n",
    "\n",
    "print(len(uris))\n",
    "print(len(audio_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "cid =\"1b81d49177e5464781a4957e5e0c1ae6\" \n",
    "secret = \"c444a35689e247f8b5f9830662bae244\" \n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret) \n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager) \n",
    "sp.trace=False \n",
    "\n",
    "uris = list(uris)\n",
    "spotify = spotipy.Spotify(auth='BQDuG3_3-tAv09LQlZKwYc-oodCcDdau9O-I-Ep_O6IK-Uqvc5S3FKwdr5qtVu5Kq1khJCwkeaR9PnQJjvL6fBRPWdjJ9H_KRoGCrSlMP5DjdcLMlWJybPF1VvJDuSwBpoxiLS_Qmr9R4z-RoDWDPNiZnlzCeJNxMMvLRg')\n",
    "\n",
    "keys_to_remove = [\"duration_ms\", \"type\", \"id\", \"uri\", \"track_href\", \"analysis_url\"]\n",
    "\n",
    "start = 0\n",
    "audio_features = []\n",
    "while start < len(uris):\n",
    "    response = sp.audio_features(uris[start:(100+start)])\n",
    "    small_response = []\n",
    "    for track in response:\n",
    "        if track is not None:\n",
    "            small_dict = {key:track[key] for key in track.keys() - keys_to_remove}\n",
    "        else:\n",
    "            print('here')\n",
    "            small_dict = {key:0.0 for key in response[0].keys() - keys_to_remove}\n",
    "        small_response.append(small_dict)\n",
    "\n",
    "    audio_features.extend(small_response)\n",
    "        \n",
    "    start += 100\n",
    "    if start % 1000 == 0: print(start)\n",
    "\n",
    "audio_df = pd.DataFrame(audio_features)\n",
    "\n",
    "\n",
    "# Save the pickles\n",
    "with open('uris_10.pickle', 'wb') as handle:\n",
    "    pickle.dump(uris, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('audio_features.pickle', 'wb') as handle:\n",
    "    pickle.dump(audio_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split:\n",
    "As well as scaling so KNN and KMeans is meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(uri_input, uri_expected, test_size=.2, random_state=41)\n",
    "\n",
    "# Scale the features in audio_df to mean=0 and variance=1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "audio_scaled = ss.fit_transform(audio_df)\n",
    "\n",
    "print(len(uris))\n",
    "print(len(audio_scaled))\n",
    "\n",
    "audio_dict = {}\n",
    "for i in range(len(uris)):\n",
    "    audio_dict[uris[i]] = audio_scaled[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and EDA\n",
    "The data sources we utilized in our explorations were the Million Playlist Dataset and the Spotify API. The MPD gave us the essential data to train models on subsets of playlists and evaluate the accuracy of our predictions. We used to Spotify API to get pre-computed audio features for songs, which allowed us to expore alternative model choices more in line with what we looked at in class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MPD EDA\n",
    "track_names = list(track_counts.keys())\n",
    "counts = list(track_counts.values())\n",
    "\n",
    "num_files = 10\n",
    "percent_data = (num_files / 1000) * 100\n",
    "\n",
    "plt.plot(range(len(track_names)), sorted(counts, reverse=True))\n",
    "plt.xlabel('Songs')\n",
    "plt.ylabel('# of Times it Appears')\n",
    "plt.title(f'Counts for each song that appears in the first {percent_data}% of the data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artist_names = list(artist_counts.keys())\n",
    "counts = list(artist_counts.values())\n",
    "\n",
    "plt.plot(range(len(artist_names)), sorted(counts, reverse=True))\n",
    "plt.xlabel('artists')\n",
    "plt.ylabel('# of Times it Appears')\n",
    "plt.title(f'Counts for each artist that appears in the first {percent_data}% of the data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(artist_names))\n",
    "print(len(track_names))\n",
    "print(np.mean(playlist_lens))\n",
    "print(np.mean(artists_perplay))\n",
    "\n",
    "plt.hist(playlist_lens, bins=30)\n",
    "plt.xlabel('# of Songs Therein')\n",
    "plt.ylabel('Playlists')\n",
    "plt.title(f'Lengths of playlists in the first {percent_data}% of the data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Median Playlist length and number of artists per playlist\n",
    "print(np.percentile(playlist_lens, q=50))\n",
    "print(np.percentile(artists_perplay, q=50))\n",
    "\n",
    "plt.hist(artists_perplay, bins=30)\n",
    "plt.xlabel('# of Artists Therein')\n",
    "plt.ylabel('Playlists')\n",
    "plt.title(f'Unique artists in playlists in the first {percent_data}% of the data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display a donut plot of the top artists\n",
    "fig, ax = plt.subplots(figsize=(12, 8), subplot_kw=dict(aspect=\"equal\"))\n",
    "\n",
    "sorted_counts = [(k, artist_counts[k]) for k in sorted(artist_counts, key=artist_counts.get, reverse=True)]\n",
    "num_to_show = 20\n",
    "\n",
    "artists = [el[0] for el in sorted_counts[:num_to_show]]\n",
    "counts = [el[1] for el in sorted_counts[:num_to_show]]\n",
    "\n",
    "wedges, texts = ax.pie(counts, wedgeprops=dict(width=0.5), startangle=-40)\n",
    "\n",
    "bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "kw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n",
    "          bbox=bbox_props, zorder=0, va=\"center\")\n",
    "\n",
    "for i, p in enumerate(wedges):\n",
    "    ang = (p.theta2 - p.theta1)/2. + p.theta1\n",
    "    y = np.sin(np.deg2rad(ang))\n",
    "    x = np.cos(np.deg2rad(ang))\n",
    "    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
    "    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n",
    "    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n",
    "    ax.annotate(artists[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n",
    "                 horizontalalignment=horizontalalignment, **kw)\n",
    "\n",
    "ax.set_title(\"Distribution of Top Artists\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs and statistics explore the features of the million playlist dataset. The first two show the number of songs that are played in the The second two show the number of unique songs and artists within playlists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More graphs and charts, both for MPD and for Spotify audio features\n",
    "col_names = ['acousticness', 'danceability', 'energy', 'instrumentalness', \n",
    "             'key', 'liveness', 'loudness', 'mode', 'speechiness', \n",
    "             'tempo', 'time_signature', 'valence']\n",
    "audio_df_scaled = pd.DataFrame(audio_scaled, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df_audio = audio_df_scaled[['liveness', 'energy', 'loudness', 'tempo']]\n",
    "pd.scatter_matrix(sub_df_audio, figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatter matrix is mainly of interest because of the relationship between energy and loudness. The two features seem to be relatively positively correlated, so we will need to be careful of colinearity when we fit models with these predictors. As for the other predictors, we can see that we have a relatively good distribution across the range, which is to be expected because we have scaled this data to have $\\mu = 0$ and $\\sigma = 1$. This should make it work well when we apply distance based techniques like KMeans and KNN to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General comparison of the scores\n",
    "fig, axes = plt.subplots(4,3, figsize=(15,20))\n",
    "axs = np.ravel(axes)\n",
    "\n",
    "for i, col_name in enumerate(col_names):\n",
    "    data = audio_df_scaled[col_name]\n",
    "    sns.distplot(data, ax=axs[i], label=col_name)\n",
    "    axs[i].set_title(f'Distribution of {col_name} values')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots are of great interest to us, as they display how our Spotify API features are distributed. This is super important when we apply these features to KMeans and KNN, as outliers or wide spreads could possibly sway the distance metrics inordinantly. However, since we have already scaled our data, most of these plots look relatively normal. There are some features (like valence and tempo) that look much more normal, while others (like loudness and liveness) that are skewed either left or right. Additionally, time signature, mode, and key appear to be discrete values, so we may have to handle those carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We explored a variety of different models and model types, but because the style of problem was different than those we studied in class, we were forced to explore different techniques than those we had seen. Our baseline models use a combination of KMeans Clustering and/or KNN, while the top two performing models are a Markov Walk on a Network and Collaborative Filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Song Based KMeans Clustering and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cluster songs and build:\n",
    "# - dict with list of URIs for each cluster\n",
    "# - dict with each song mapped to its cluster\n",
    "n_clusters = 75\n",
    "km_songs = KMeans(n_clusters=n_clusters)\n",
    "song_clusters = km_songs.fit_predict(audio_scaled)\n",
    "\n",
    "cluster_to_songs, song_to_cluster = {}, {}\n",
    "for i, cluster_num in enumerate(song_clusters):\n",
    "    if cluster_num not in cluster_to_songs:\n",
    "        cluster_to_songs[cluster_num] = []\n",
    "        \n",
    "    cluster_to_songs[cluster_num].append(uris[i])\n",
    "    song_to_cluster[uris[i]] = cluster_num\n",
    "    \n",
    "# Save the pickles\n",
    "with open('cluster_to_songs.pickle', 'wb') as handle:\n",
    "    pickle.dump(cluster_to_songs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('song_to_cluster.pickle', 'wb') as handle:\n",
    "    pickle.dump(song_to_cluster, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the pickles instead of rescraping\n",
    "with open('./cs109_final_backend/cs109_final_backend/cluster_files/cluster_to_songs.pickle', 'rb') as f:    \n",
    "    cluster_to_songs = pickle.load(f)\n",
    "with open('./cs109_final_backend/cs109_final_backend/cluster_files/song_to_cluster.pickle', 'rb') as f:    \n",
    "    song_to_cluster = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:  \n",
    "This clustering model is very much a baseline for the other models. Intuitively, it clusters all songs then for each input of seed songs, finds an the 'closest' songs to those, in some way. They all use the Spotify API audio features to cluster and make predictions, relatively ignoring the MPD aside as input and output uris. The following class has three different predict methods, namely `predict`, `predict2`, and `predict3`. The first simply calculates the most populous cluster among the input data, then randomly samples 500 songs from that cluster. The second tries to match the distribution of input songs more closely, outputting the number of songs in the input per cluster scaled up for a total of 500. Finally, the last method uses the audio features even more, calculating the 'distance' of every song in the same cluster to the 'average' of the input songs, outputting the 500 songs closest to the average in order. Overall, these methods do fairly poorly at matching the held out songs, only retrieving relevant songs fairly rarely. The R-precision of these methods is in the range of 0.005-0.011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClusterModel:\n",
    "    def __init__(self, cluster_to_song, song_to_cluster, audio_dict=None, n_clusters=20, K=25):\n",
    "        self.name = 'cluster_model'\n",
    "        self.n_clusters = n_clusters\n",
    "        self.cluster_to_song = cluster_to_song\n",
    "        self.song_to_cluster = song_to_cluster\n",
    "        self.K = K\n",
    "        self.audio_dict = audio_dict\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for playlist in X:\n",
    "            clusters = [self.song_to_cluster[song] for song in playlist if song in self.song_to_cluster]\n",
    "            unique, counts = np.unique(clusters, return_counts=True)\n",
    "            \n",
    "            if len(unique) == 0:\n",
    "                max_cluster_id = np.random.randint(0, len(cluster_to_songs))\n",
    "            else:\n",
    "                max_cluster_id = unique[np.argmax(counts)]\n",
    "                \n",
    "            max_cluster = self.cluster_to_song[max_cluster_id]\n",
    "            max_cluster = self.cluster_to_song[max_cluster_id]\n",
    "            try:\n",
    "                predicted = np.random.choice(max_cluster, size=500, replace=False)\n",
    "            except ValueError:\n",
    "                predicted = max_cluster\n",
    "            predictions.append(predicted)\n",
    "        return predictions\n",
    "    \n",
    "    def predict2(self, X):\n",
    "        predictions = []\n",
    "        for playlist in X:\n",
    "            clusters = [self.song_to_cluster[song] for song in playlist if song in self.song_to_cluster]\n",
    "            unique, counts = np.unique(clusters, return_counts=True)\n",
    "            predicted = []\n",
    "            for cl, count in zip(unique, counts):\n",
    "                size = count * (500 // self.K)\n",
    "                cluster = self.cluster_to_song[cl]\n",
    "                preds = np.random.choice(cluster, size=size, replace=False)\n",
    "                predicted.extend(preds)\n",
    "            predictions.append(predicted)\n",
    "        return predictions\n",
    "    \n",
    "    def predict3(self, X):\n",
    "        assert(self.audio_dict is not None)\n",
    "        predictions = []\n",
    "        for playlist in X:\n",
    "            clusters = [self.song_to_cluster[song] for song in playlist if song in self.song_to_cluster]\n",
    "            unique, counts = np.unique(clusters, return_counts=True)\n",
    "            \n",
    "            if len(unique) == 0:\n",
    "                max_cluster_id = np.random.randint(0, len(cluster_to_songs))\n",
    "            else:\n",
    "                max_cluster_id = unique[np.argmax(counts)]\n",
    "            max_cluster = self.cluster_to_song[max_cluster_id]\n",
    "            \n",
    "            avg_feat = self.get_average_features(playlist)\n",
    "            distances = [(uri, self.distance(avg_feat, self.audio_dict[uri])) for uri in max_cluster]\n",
    "            distances.sort(key=lambda tup: tup[1])\n",
    "            \n",
    "            predictions.append([uri for uri, _ in distances[:500]])\n",
    "        return predictions\n",
    "    \n",
    "    def get_average_features(self, playlist):\n",
    "        average_features = None\n",
    "        for uri in playlist:\n",
    "            features = self.audio_dict[uri]\n",
    "            if average_features is None:\n",
    "                average_features = features\n",
    "            else:\n",
    "                average_features = average_features + features\n",
    "        average_features = average_features / len(playlist)\n",
    "        return average_features\n",
    "    \n",
    "    def distance(self, audio1, audio2):\n",
    "        distance = np.sqrt(np.sum((audio1 - audio2) ** 2.0))\n",
    "        return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another baseline: Playlist Based KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KNNModel():\n",
    "    def __init__(self, K=5):\n",
    "        self.K = K\n",
    "    \n",
    "    def fit(self, playlists):\n",
    "        self.playlists = playlists\n",
    "        self.songs = set([y for x in self.playlists for y in x])\n",
    "    \n",
    "        self.mlb = MultiLabelBinarizer(classes=list(self.songs))\n",
    "        self.matrix = self.mlb.fit_transform(self.playlists)\n",
    "  \n",
    "    def recommendations(self, tracks, n_recs):\n",
    "        known_tracks = [track for track in tracks if track in self.songs]\n",
    "        vector = self.mlb.transform([known_tracks])[0]\n",
    "        neigh = NearestNeighbors(self.K, algorithm='brute', metric='cosine')\n",
    "        neigh.fit(self.matrix)\n",
    "        kneighbors = neigh.kneighbors([vector])\n",
    "    \n",
    "        recs = []\n",
    "        for i in kneighbors[1][0]:\n",
    "            recs += self.playlists[i]\n",
    "            \n",
    "        for uri in tracks:\n",
    "            if uri in recs:\n",
    "                recs.remove(uri)\n",
    "        \n",
    "        return [uri for uri,_ in Counter(recs).most_common(n_recs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K Nearest Neighbors algorithm gives another basic method for making recommendations. In laymans terms, the model simply finds the pre-existing playlists most similar to a set of given tracks and then gives the tracks on those playlists as recommendations. To do this, we create an $N*M$ binary matrix $Q$ representing playlist membership where $N$ is the number of playlists given to train the model and $M$ is the total number of unique songs within those playlists. If playlist $i$ contains song $j$, $Q_{i,j}=1$. Every other entry is $0$. To generate predictions from a list of tracks, the model converts the list to this format and finds the most similar $k$ playlists in terms cosine distance. All of the tracks from these playlists are counted and returned by rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filtering code here (Chris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Based Markov Model\n",
    "\n",
    "This model is a probabilistic one that builds up a network where each vertex represents a song and each edge represents two songs sharing a playlist (where more shared playlists leads to higher weighting). Then for prediction, for each of the input $K$ songs, many one step random walks are taken, and the most popular songs that show up in these walks are then returned as a list of 500 song recommendations (after getting rid of duplicates that are already in the playlist). This model consistently performed the best of our models, although building up a large network takes a significant amount of time and space.\n",
    "\n",
    "**Motivation:** The motivation for using a Network/Markov Chain approach was that people will want to put songs together into playlists in a similar manner that other people have put songs together into playlists. Thus, looking at what songs are normally put into playlists together, and how often, should be a good indication of what songs will be put into playlists together at a later date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Building code\n",
    "\n",
    "NETWORK_FILE_PATH = './cs109_final_backend/cs109_final_backend/network_files/pickled_network.pickle'\n",
    "\n",
    "with open(NETWORK_FILE_PATH, 'rb') as f: \n",
    "    NETWORK = pickle.load(f)\n",
    "    \n",
    "def n_top_songs(playlist_songs, network, num_samples=4000, num_top_songs=500): \n",
    "    \n",
    "    # for if we need to fill in with random songs... (see keyerror)\n",
    "    all_songs = list(network)\n",
    "    \n",
    "    key_errors = 0\n",
    "    all_samples = np.array([])\n",
    "    for song_uri in playlist_songs: \n",
    "        try : \n",
    "            sample = np.random.choice(network[song_uri]['songs'], num_samples, p=network[song_uri]['counts'])\n",
    "            all_samples = np.append(all_samples, sample)\n",
    "        except KeyError: \n",
    "            # if we get a key error, just randomly choose 1000 songs and add them to the samples\n",
    "            # this could be fixed with a larger network / training set that has \n",
    "            # every song on at least one playlist... for now lets use randomness\n",
    "            key_errors += 1\n",
    "            all_samples = np.append(all_samples, random.sample(all_songs, int(num_samples/4)))\n",
    "        \n",
    "        \n",
    "    unique, counts = np.unique(all_samples, return_counts=True)\n",
    "    \n",
    "    counts = counts.astype(float) / np.sum(counts)\n",
    "    counted_samples = zip(unique, counts)\n",
    "    counted_samples = [sample for sample in counted_samples if sample[0] not in playlist_songs]\n",
    "    counted_samples = sorted(counted_samples, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    num_to_return = min(num_top_songs, len(counted_samples))\n",
    "\n",
    "    return counted_samples[:num_to_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_network_accuracy(train, test, network, num_predictions=500): \n",
    "    print (\"starting with {} songs, and trying to find {} songs\".format(len(train), len(test)))\n",
    "    preds = n_top_songs(train, network, num_top_songs = num_predictions)\n",
    "    preds = [p[0] for p in preds]\n",
    "    correct_ratio = len([x for x in preds if x in test])/(1. * len(test))\n",
    "    print(correct_ratio)\n",
    "    return correct_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "We decided to evaluate our models based on the same metrics used in the Spotify RecSys [contest rules](https://recsys-challenge.spotify.com/rules), namely R-Precision (RPrec), Normalized Discounted Cumulative Gain (NDCG), and Recommended Song Clicks (RSC). In the following definitions, $G$ is the set of ground truth tracks representing the held out songs from each playlist and $R$ is the ordered list of recommended songs returned by the recommendation system.\n",
    "\n",
    "* R-Precision: The metric counts \"number of retrieved relevant tracks divided by the number of known relevant tracks,\" rewarding the total number of retrieved relevant tracks, regardless of order.\n",
    "$$\\text{R-precision} = \\frac{\\left| G \\cap R_{1:|G|} \\right|}{|G|}$$\n",
    "\n",
    "* Normalized Discounted Cumulative Gain (NDCG): This metric takes into account the order of the returned songs, rewarding relevant songs placed higher in the returned list. It is calculated as Discounted Cumulative Gain (DCG), divided by the Ideal Discounted Cumulative Gain (IDCG), where the returned songs are ordered perfectly. That calculation looks like:\n",
    "$$DCG = rel_1 + \\sum_{i=2}^{|R|} \\frac{rel_i}{\\log_2 (i + 1)}$$\n",
    "$$IDCG = 1 + \\sum_{i=2}^{|G|} \\frac{1}{\\log_2 (i + 1)}$$\n",
    "$$NDCG = \\frac{DCG}{IDCG}$$\n",
    "\n",
    "* Recommended Songs Clicks (RSC): This measures how many \"clicks\" a Spotify user would need to find the first relevant song in the recommendations (the first song actually in the rest of the playlist $G$), where Spotify displays recommended songs in groups of 10. Therefore it's simply finding the first relevant song and returning its position in the list divided by 10 and truncated. Or more formally:\n",
    "$$\\text{clicks} = \\left\\lfloor \\frac{ \\arg\\min_i \\{ R_i\\colon R_i \\in G|\\} - 1}{10} \\right\\rfloor$$\n",
    "\n",
    "We have implemented these metrics in code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "class Evaluator():\n",
    "    \"\"\"Superclass for evaluation functions\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def evaluate(self, output, expected):\n",
    "        \"\"\"\n",
    "        Output will be the output of the model for some list of playlists\n",
    "        - Shape of (# playlists, 500)\n",
    "\n",
    "        Expected will be the held out songs from each playlist\n",
    "        - List of lists of various sizes\n",
    "\n",
    "        Note: Each \"song\" will be the unique spotify uri of a song\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class RPrecision(Evaluator):\n",
    "    \"\"\"\n",
    "    R-precision measures the number of held out songs correctly \n",
    "        retrieved by the model output \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Evaluator.__init__(self, 'R-Precision')\n",
    "        \n",
    "    def evaluate(self, output, expected, return_all=False):\n",
    "    \n",
    "        def rprec_one(output_, expected_):\n",
    "            expected_size = len(expected_)\n",
    "            common_set = set(output_).intersection(set(expected_))\n",
    "            common_size = len(common_set)\n",
    "            if expected_size == 0 or common_size == 0:\n",
    "                return 0.0\n",
    "            return 1. * common_size / expected_size\n",
    "        \n",
    "        r_precs = [rprec_one(out, exp) for (out, exp) in zip(output, expected)]\n",
    "        if return_all:\n",
    "            return np.mean(r_precs), r_precs\n",
    "        return np.mean(r_precs)\n",
    "\n",
    "    \n",
    "class NDCG(Evaluator):\n",
    "    \"\"\"\n",
    "    Normalized discounted cumulative gain also takes into \n",
    "        account how the system ordered the suggestions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Evaluator.__init__(self, 'NDCG')\n",
    "        \n",
    "    def evaluate(self, output, expected, return_all=False):\n",
    "        \n",
    "        def ndcg_one(output_, expected_):\n",
    "            dcg, idcg = 0.0, 0.0\n",
    "            \n",
    "            if len(output_) == 0 or len(expected_) == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            expected_ = set(expected_)\n",
    "            for i in range(len(output_)):\n",
    "                # Prediction DCG\n",
    "                if output_[i] in expected_:\n",
    "                    if i == 0:\n",
    "                        dcg += 1.0\n",
    "                    else:\n",
    "                        dcg += 1.0 / log2(i + 2.0)\n",
    "\n",
    "                if i < len(expected_):\n",
    "                    if i == 0:\n",
    "                        idcg += 1.0\n",
    "                    else:\n",
    "                        idcg += 1.0 / log2(i + 2.0)\n",
    "            \n",
    "            return dcg / idcg\n",
    "        \n",
    "        precs = [ndcg_one(out, exp) for (out, exp) in zip(output, expected)]\n",
    "        if return_all : \n",
    "            return np.mean(precs), precs\n",
    "        else : \n",
    "            return precs\n",
    "        \n",
    "        \n",
    "class RSC(Evaluator):\n",
    "    \"\"\"\n",
    "    Recommended Song Clicks measures how many times a user\n",
    "    would have to click through the suggestions to find a song that \n",
    "    was a ground truth song\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Evaluator.__init__(self, 'RSC')\n",
    "        \n",
    "    def evaluate(self, output, expected, return_all=False):\n",
    "        \n",
    "        def rsc_one(output_, expected_):\n",
    "            if len(output_) == 0 or len(expected_) == 0:\n",
    "                return 51\n",
    "            \n",
    "            output_len = len(output_)\n",
    "            expected_ = set(expected_)\n",
    "            for i in range(output_len):\n",
    "                if output_[i] in expected_:\n",
    "                    return i//10\n",
    "            return 51\n",
    "        \n",
    "        scores = [rsc_one(out, exp) for (out, exp) in zip(output, expected)]\n",
    "        if return_all : \n",
    "            return np.mean(scores), scores\n",
    "        else : \n",
    "            return np.mean(scores)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(output, expected, title=''):\n",
    "    r_prec = RPrecision().evaluate(output, expected)\n",
    "    ndcg = NDCG().evaluate(output, expected)\n",
    "    rsc = RSC().evaluate(output, expected)\n",
    "    print(\"{}: R-Precision: {}, NCDG: {}, RSC: {}\".format(title, r_prec, ndcg, rsc))\n",
    "    \n",
    "def build_evaluation_dataset(start, blocks = 1, n_predictors=10, min_remaining = 100, max_remaining = 125) : \n",
    "    \"\"\" Build a list of first n song lists, and a list of last total - n song lists \n",
    "    \n",
    "    Args: \n",
    "        start : (int) the starting playlist slice\n",
    "        blocks : (int) The number of playlist slices to use\n",
    "        n_predictor : (int) The number of songs to be in the list of predictor lists\n",
    "        min_remaining : (int) The minimum number of songs remaining on the playlist\n",
    "        max_remaining : (int) The maximum number of songs remaining on the playlist\n",
    "        \n",
    "    Returns: \n",
    "        predictor_songs : ((str list) list) List of predictor song lists\n",
    "        remainder_songs : ((str list) list) List of remaining songs (the ones we're trying to guess)\n",
    "    \n",
    "    \"\"\"\n",
    "    f_start = start * 1000\n",
    "    f_end = start * 1000 + 999\n",
    "    predictor_songs = []\n",
    "    remainder_songs = []\n",
    "    for i in range(blocks): \n",
    "        with open('./mpd.v1/data/mpd.slice.{}-{}.json'.format(f_start, f_end)) as f :\n",
    "            data = json.load(f)\n",
    "            \n",
    "            for playlist in data['playlists'] : \n",
    "                tracks = [t['track_uri'] for t in playlist['tracks']]\n",
    "                if len(tracks) >= min_remaining + n_predictors and len(tracks) <= max_remaining + n_predictors: \n",
    "                    predict = tracks[:n_predictors]\n",
    "                    remain = tracks[n_predictors:]\n",
    "\n",
    "                    predictor_songs.append(predict)\n",
    "                    remainder_songs.append(remain)\n",
    "            \n",
    "            \n",
    "    return predictor_songs, remainder_songs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of each model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Baseline Clustering**\n",
    "We follow similar evaluation to the Network Evaluation, on the same test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracies_from_cluster(k, start_block, cluster_to_songs, \n",
    "                                song_to_cluster, audio_dict, blocks=10, \n",
    "                                min_remaining=50, max_remaining=200): \n",
    "    \"\"\" Builds the prediction/remainder sets for a given k starting \n",
    "        at slice=start_block and returns a list of the scores based on each metric for \n",
    "        each test playlist\n",
    "        \n",
    "    Args: \n",
    "        k : (int) Number of predictor songs\n",
    "        start_block : (int) The slice number to start at\n",
    "        cluster_to_songs : (dict) Mapping each cluster to the songs in that cluster\n",
    "        song_to_cluster : (dict) Reverse mapping from songs -> cluster\n",
    "        audio_dict : (dict) Mapping uri -> audio features\n",
    "        blocks : (int) The number of slices to read\n",
    "        min_remaining : (int) The min number of remaining tracks to allow\n",
    "        max_remaining : (int) The max number of remaining tracks to allow\n",
    "        \n",
    "    Returns: \n",
    "        r2_results : (float list) The Rprec results for each playlist\n",
    "        ndcg_results : (float list) The NDCG results for each playlist\n",
    "        rsc_results : (float list) The click scores for each playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # build the prediction/remainder data\n",
    "    predictors, remainders = build_evaluation_dataset(start_block, blocks=blocks, \n",
    "                                                      n_predictors=k, min_remaining=min_remaining, \n",
    "                                                      max_remaining=max_remaining)\n",
    "    \n",
    "    \n",
    "    # get the predictions from the network\n",
    "    cm = ClusterModel(cluster_to_songs, song_to_cluster, audio_dict)\n",
    "    predictions = cm.predict(predictors)\n",
    "    \n",
    "    # evaluate the model based on the 3 metrics \n",
    "    r_prec = RPrecision()\n",
    "    r2_results = r_prec.evaluate(predictions, remainders, return_all=True)[1]\n",
    "    \n",
    "    ndcg_eval = NDCG()\n",
    "    ndcg_results = ndcg_eval.evaluate(predictions, remainders, return_all=True)[1]\n",
    "    \n",
    "    rsc_eval = RSC()\n",
    "    rsc_results = rsc_eval.evaluate(predictions, remainders, return_all=True)[1]\n",
    "    \n",
    "    \n",
    "    return r2_results, ndcg_results, rsc_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r2_acc, nd_acc, rsc_res = get_accuracies_from_cluster(5, 10, cluster_to_songs, \n",
    "                                song_to_cluster, audio_dict, blocks=2)\n",
    "\n",
    "r2s = []\n",
    "nds = []\n",
    "rscs = []\n",
    "table = PrettyTable()\n",
    "table.field_names = ['K', 'Mean RPrec', 'Mean NCDG', 'Mean Clicks']\n",
    "for k in [1,5,10,25,100] : \n",
    "    r2_acc, nd_acc, rsc_res = get_accuracies_from_cluster(k, 100, cluster_to_songs, \n",
    "                                song_to_cluster, audio_dict, blocks=1)\n",
    "    r2s.append(r2_acc)\n",
    "    nds.append(nd_acc)\n",
    "    rscs.append(rsc_res)\n",
    "    \n",
    "    table.add_row([k, round(np.mean(r2_acc), 4), round(np.mean(nd_acc),4), round(np.mean(rsc_res),4)])\n",
    "    \n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [1,5,10,25, 100]\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,5))\n",
    "\n",
    "sns.lineplot(x, [np.mean([x]) for x in r2s], ax=ax1)\n",
    "ax1.set_title('Mean RPrec Scores vs K')\n",
    "ax1.set_xlabel('K')\n",
    "ax1.set_ylabel('Mean RPrec')\n",
    "\n",
    "sns.lineplot(x, [np.mean([x]) for x in nds], ax=ax2)\n",
    "ax2.set_title('Mean NDCG Scores vs K')\n",
    "ax2.set_xlabel('K')\n",
    "ax2.set_ylabel('Mean NDCG Score')\n",
    "\n",
    "sns.lineplot(x, [np.mean([x]) for x in rscs], ax=ax3)\n",
    "ax3.set_title('Mean Clicks vs K')\n",
    "ax3.set_xlabel('K')\n",
    "ax3.set_ylabel('Mean Clicks')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this model is meant to be very much a baseline, these metrics confirm our assumption that it does only a little better than random chance. While it regularly finds some songs (generally no more than 1 or 2) from the held out songs, the random chance model we tested nearly never retrieved a relevant song. We thought this would be a good baseline as well as giving us a chance to apply some models we learned in class, whereas the two better models we implemented were extensions that we didn't cover at all this semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2. Network: \n",
    "\n",
    "Letting $K$ be the number of seed songs, and $S$ be the number of remaining songs that we are trying to predict, we will evaluate the network model as follows. \n",
    "\n",
    "We will use $k = [1,5,10,25,100]$ (spotify challenge requirements) while setting $ 25 \\leq S \\leq 200$ in order to keep the number of tracts to predict slightly more consistant in order to ensure that the changing numer of tracks doesn't affect the accuracy as much as with an even larger range. The number of predicted songs will be a constant 500 as in the official RecSys Challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracies_from_network(k, network, start_block, blocks=10, min_remaining=25, max_remaining=200, \n",
    "                                num_samples = 1000, num_top_songs=500): \n",
    "    \"\"\" Builds the prediction/remainder sets for a given k starting \n",
    "        at slice=start_block and returns a list of the scores based on each metric for \n",
    "        each test playlist\n",
    "        \n",
    "    Args: \n",
    "        k : (int) Number of predictor songs\n",
    "        start_block : (int) The slice number to start at\n",
    "        blocks : (int) The number of slices to read\n",
    "        min_remaining : (int) The min number of remaining tracks to allow\n",
    "        max_remaining : (int) The max number of remaining tracks to allow\n",
    "        num_samples  : (int) The number of samples to take from each predictor track\n",
    "        num_top_songs : (int) The number of song predictions to return \n",
    "        \n",
    "    Returns: \n",
    "        r2_results : (float list) The Rprec results for each playlist\n",
    "        ndcg_results : (float list) The NDCG results for each playlist\n",
    "        rsc_results : (float list) The click scores for each playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # build the prediction/remainder data\n",
    "    predictors, remainders = build_evaluation_dataset(start_block, blocks=blocks, \n",
    "                                                      n_predictors=k, min_remaining=min_remaining, \n",
    "                                                      max_remaining=max_remaining)\n",
    "    \n",
    "    \n",
    "    # get the predictions from the network\n",
    "    predictions = []\n",
    "    for i in range(len(predictors)): \n",
    "        p = [s[0] for s in n_top_songs(predictors[i], NETWORK, num_samples=num_samples, num_top_songs=num_top_songs)]\n",
    "        predictions.append(p)\n",
    "    \n",
    "    # evaluate the model based on the 3 metrics \n",
    "    r_prec = RPrecision()\n",
    "    r2_results = r_prec.evaluate(predictions, remainders, return_all=True)[1]\n",
    "    \n",
    "    ndcg_eval = NDCG()\n",
    "    ndcg_results = ndcg_eval.evaluate(predictions, remainders, return_all=True)[1]\n",
    "    \n",
    "    rsc_eval = RSC()\n",
    "    rsc_results = rsc_eval.evaluate(predictions, remainders, return_all=True)[1]\n",
    "    \n",
    "    \n",
    "    return r2_results, ndcg_results, rsc_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sns.distplot(get_accuracies_from_network(5, NETWORK, 100))\n",
    "# Evaluate the network model for the given k values, using 2 1000 playlist \n",
    "# slices, starting at slice 100. (The model was build from slices 2-15)\n",
    "r2s = []\n",
    "nds = []\n",
    "rscs = []\n",
    "table = PrettyTable()\n",
    "table.field_names = ['K', 'Mean RPrec', 'Mean NCDG', 'Mean Clicks']\n",
    "for k in [1,5,10,25, 100] : \n",
    "    r2_acc, nd_acc, rsc_res = get_accuracies_from_network(k, NETWORK, 100, blocks=2)\n",
    "    r2s.append(r2_acc)\n",
    "    nds.append(nd_acc)\n",
    "    rscs.append(rsc_res)\n",
    "    \n",
    "    table.add_row([k, round(np.mean(r2_acc), 4), round(np.mean(nd_acc),4), round(np.mean(rsc_res),4)])\n",
    "    \n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can start to see trends in the various evaluation methods. It's important to note that because of constraints on local processing power, the network does not necessarily have nodes for all of the test songs which is hurting performance, but a natural drawback of the Markov-chain approach: namely the model itself is quite large. We counter this by randomly sampling songs from the network with equal weight whenever a seed song is not in the network. \n",
    "\n",
    "Looking at the performance of the network approach based on the three scoring systems, we can see that the model seems to perform best for the $RPrec$ score and $NCDG$ score methods using $K=25$ seed songs, while the mean clicks required to find a relevant song is best with $K=100$ seed songs. \n",
    "\n",
    "Furthermore, while we do not have access to the official test sets used by Spotify, we can start to see the benefits and drawbacks of the Markov-chain based model. The Markov-chain based model seems to perform best (relative to the other metrics) on the $RPrec$ score. The best performing Spotify challenge contestant had a score of $RPrec = .224$ on the official test set, whereas, depending on the $K$ value, our model had a mean $RPrec \\in \\{0.17,0.28, 0.32, 0.28\\}$. This should of course be taken with a grain of salt as our model most likely would not have had the best performance in the challenge, but does signal that it is a reasonably good approach. \n",
    "\n",
    "Moving on to mean NCDG score, which takes into account the actual ranking of importance of the predictions into account, our model falls much closer to the middle of the pack in the RecSys leaderboards (again, this is a tough comparison given that we don't have the test set used in the challenge). With scores in the low $0.2$ range, (roughly ~60th/110 in the actual challenge) we can start to see the drawbacks of the Markov model. \n",
    "\n",
    "Lastly our mean Click Score again places the model in the relative middle of the pack for the RecSys leaderboards. Where exactly is unclear because there is no info on the test data and what the distribution of $K$ was, however, it seems that the Markov-chain model can hold its own. \n",
    "\n",
    "But why does the Markov-chain model apparently do so much better with the $RPrec$ scoring method than the others? At its base, this makes sense because the whole idea of the model is that tracks that people put together on their own will likely be put together by people again. The one step random walk should do a fairly good job of getting the most likely songs as a set, but the ranking system of appearances in the sample set does not seem to perform as well as the other Spotify challenge methods for ensuring the most relevant songs are given first. This would suggest a need to work on the weighting system used by the algorithm. The mean Clicks Score also shows the issues as with the NCDG score, namely the member songs are not always ranked as the most relevant. \n",
    "\n",
    "All and all, however, it seems like this model would do an ok job of keeping up with the pack in the actual RecSys challenge based on the limited information that we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [1,5,10,25, 100]\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,5))\n",
    "\n",
    "sns.lineplot(x, [np.mean([x]) for x in r2s], ax=ax1)\n",
    "ax1.set_title('Mean RPrec Scores vs K')\n",
    "ax1.set_xlabel('K')\n",
    "ax1.set_ylabel('Mean RPrec')\n",
    "\n",
    "sns.lineplot(x, [np.mean([x]) for x in nds], ax=ax2)\n",
    "ax2.set_title('Mean NDCG Scores vs K')\n",
    "ax2.set_xlabel('K')\n",
    "ax2.set_ylabel('Mean NDCG Score')\n",
    "\n",
    "sns.lineplot(x, [np.mean([x]) for x in rscs], ax=ax3)\n",
    "ax3.set_title('Mean Clicks vs K')\n",
    "ax3.set_xlabel('K')\n",
    "ax3.set_ylabel('Mean Clicks')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above:** Here we can see the same mean scores discussed above, but in a graphical form. The R2 and NDCG scores seem to improve as $K$ (the number of seed tracks) increases, and then decreases. While the Mean Clicks score improves, then gets worse, and then improves again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# General comparison of the scores\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "sns.kdeplot(r2s[0], label='K=1', ax=ax1)\n",
    "sns.kdeplot(r2s[1], label='K=5', ax=ax1)\n",
    "sns.kdeplot(r2s[2], label='K=10', ax=ax1)\n",
    "sns.kdeplot(r2s[3], label='K=25', ax=ax1)\n",
    "sns.kdeplot(r2s[4], label='K=100', ax=ax1)\n",
    "ax1.set_title('Distribution of RPrec Scores by K')\n",
    "\n",
    "sns.kdeplot(nds[0], label='K=1', ax=ax2)\n",
    "sns.kdeplot(nds[1], label='K=5', ax=ax2)\n",
    "sns.kdeplot(nds[2], label='K=10', ax=ax2)\n",
    "sns.kdeplot(nds[3], label='K=25', ax=ax2)\n",
    "sns.kdeplot(nds[4], label='K=100', ax=ax2)\n",
    "ax2.set_title('Distribution of NDCG Scores by K')\n",
    "\n",
    "sns.kdeplot(rscs[0], label='K=1', ax=ax3)\n",
    "sns.kdeplot(rscs[1], label='K=5', ax=ax3)\n",
    "sns.kdeplot(rscs[2], label='K=10', ax=ax3)\n",
    "sns.kdeplot(rscs[3], label='K=25', ax=ax3)\n",
    "sns.kdeplot(rscs[4], label='K=100', ax=ax3)\n",
    "ax3.set_title('Distribution of Click Scores by K')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above:** Here we can see the distribution of each of the three metrics colored by $K$. Though hard ot pick out the indivual distributions, it shows how they move together in general. To look at something a little easier to interpret, lets look at the distributions for each scoring metric by $K$ below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ks = [1,5,10,25,100]\n",
    "fig, axes = plt.subplots(5,3, figsize=(15,30))\n",
    "for i in range(5) :\n",
    "    ax1 = axes[i,0]\n",
    "    ax2 = axes[i,1]\n",
    "    ax3 = axes[i,2]\n",
    "    \n",
    "    sns.distplot(r2s[i], ax=ax1)\n",
    "    ax1.set_title('RPrec Scores | K = {}'.format(ks[i]))\n",
    "    ax1.set_xlabel('RPrec Score')\n",
    "    \n",
    "    sns.distplot(nds[i], ax=ax2)\n",
    "    ax2.set_title('NDCG Scores | K = {}'.format(ks[i]))\n",
    "    ax2.set_xlabel('NDCG Score')\n",
    "    \n",
    "    sns.distplot(rscs[i], ax=ax3)\n",
    "    ax3.set_title('Click Scores | K = {}'.format(ks[i]))\n",
    "    ax3.set_xlabel('Required Number of Clicks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above:** In this more in-depth view of the scoring methods by $K$ value we can see the mean trends discussed earlier but in more detail. In terms of $RPrec$ score we can watch the distribution shift right showing the improved RPrec score as $K$ increase, but then shift left again once $K=100$. We see a similar result in the $NDCG$ scores. Interestingly we can see in the click scores that there are two main 'bumps' in the distribution. The first is around 1, showing that most times there is relevant song on the first page or in the first couple pages for the most part. Then there is another bump around 51 (the max value allowed per the SysRec evaluation specs), which shows that sometimes the reccomendations contain none of the expected songs. As $K$ increases, however, we can see that the bump at 1 grows higher and higher, while the bump at around 51 shrinks showing the overal improvement in the predictions (based on click score) as $K$ increases. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. Playlist based KNN **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracies_from_KNN(k, knn_model, start_block, blocks=10, min_remaining=25, max_remaining=200, num_top_songs=500):\n",
    "    \"\"\" Builds the prediction/remainder sets for a given k starting \n",
    "        at slice=start_block and returns a list of the scores based on each metric for \n",
    "        each test playlist\n",
    "        \n",
    "    Args: \n",
    "        k : (int) Number of predictor songs\n",
    "        start_block : (int) The slice number to start at\n",
    "        blocks : (int) The number of slices to read\n",
    "        min_remaining : (int) The min number of remaining tracks to allow\n",
    "        max_remaining : (int) The max number of remaining tracks to allow\n",
    "        num_samples  : (int) The number of samples to take from each predictor track\n",
    "        num_top_songs : (int) The number of song predictions to return \n",
    "        \n",
    "    Returns: \n",
    "        r2_results : (float list) The Rprec results for each playlist\n",
    "        ndcg_results : (float list) The NDCG results for each playlist\n",
    "        rsc_results : (float list) The click scores for each playlist\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    predictors, remainders = build_evaluation_dataset(start_block, blocks=blocks, \n",
    "                                                      n_predictors=k, min_remaining=min_remaining, \n",
    "                                                      max_remaining=max_remaining)\n",
    "    \n",
    "    predictions = []\n",
    "    for predictor in predictors:\n",
    "        p = knn_model.recommendations(predictor,num_top_songs)\n",
    "        predictions.append(p)\n",
    "        \n",
    "    r_prec = RPrecision()\n",
    "    r2_results = r_prec.evaluate(predictions, remainders, return_all=True)[1]\n",
    "    \n",
    "    ndcg_eval = NDCG()\n",
    "    ndcg_results = ndcg_eval.evaluate(predictions, remainders, return_all=True)[1]\n",
    "    \n",
    "    rsc_eval = RSC()\n",
    "    rsc_results = rsc_eval.evaluate(predictions, remainders, return_all=True)[1]\n",
    "    \n",
    "    \n",
    "    return r2_results, ndcg_results, rsc_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "playlists = build_evaluation_dataset(80, blocks=10,n_predictors=1, min_remaining=25, max_remaining=200)[0]\n",
    "knn_model = KNNModel(100)\n",
    "knn_model.fit(playlists)\n",
    "\n",
    "r2s = []\n",
    "nds = []\n",
    "rscs = []\n",
    "table = PrettyTable()\n",
    "table.field_names = ['K', 'Mean RPrec', 'Mean NCDG', 'Mean Clicks']\n",
    "for k in [1,5,10,25, 100] : \n",
    "    r2_acc, nd_acc, rsc_res = get_accuracies_from_KNN(k, knn_model, 100, blocks=1)\n",
    "    r2s.append(r2_acc)\n",
    "    nds.append(nd_acc)\n",
    "    rscs.append(rsc_res)\n",
    "    \n",
    "    table.add_row([k, round(np.mean(r2_acc), 4), round(np.mean(nd_acc),4), round(np.mean(rsc_res),4)])\n",
    "    \n",
    "\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [1,5,10,25, 100]\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,5))\n",
    "\n",
    "sns.lineplot(x, [np.mean([x]) for x in r2s], ax=ax1)\n",
    "ax1.set_title('Mean RPrec Scores vs K')\n",
    "ax1.set_xlabel('K')\n",
    "ax1.set_ylabel('Mean RPrec')\n",
    "\n",
    "sns.lineplot(x, [np.mean([x]) for x in nds], ax=ax2)\n",
    "ax2.set_title('Mean NDCG Scores vs K')\n",
    "ax2.set_xlabel('K')\n",
    "ax2.set_ylabel('Mean NDCG Score')\n",
    "\n",
    "sns.lineplot(x, [np.mean([x]) for x in rscs], ax=ax3)\n",
    "ax3.set_title('Mean Clicks vs K')\n",
    "ax3.set_xlabel('K')\n",
    "ax3.set_ylabel('Mean Clicks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# General comparison of the scores\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "sns.kdeplot(r2s[0], label='K=1', ax=ax1)\n",
    "sns.kdeplot(r2s[1], label='K=5', ax=ax1)\n",
    "sns.kdeplot(r2s[2], label='K=10', ax=ax1)\n",
    "sns.kdeplot(r2s[3], label='K=25', ax=ax1)\n",
    "sns.kdeplot(r2s[4], label='K=100', ax=ax1)\n",
    "ax1.set_title('Distribution of RPrec Scores by K')\n",
    "\n",
    "sns.kdeplot(nds[0], label='K=1', ax=ax2)\n",
    "sns.kdeplot(nds[1], label='K=5', ax=ax2)\n",
    "sns.kdeplot(nds[2], label='K=10', ax=ax2)\n",
    "sns.kdeplot(nds[3], label='K=25', ax=ax2)\n",
    "sns.kdeplot(nds[4], label='K=100', ax=ax2)\n",
    "ax2.set_title('Distribution of NDCG Scores by K')\n",
    "\n",
    "sns.kdeplot(rscs[0], label='K=1', ax=ax3)\n",
    "sns.kdeplot(rscs[1], label='K=5', ax=ax3)\n",
    "sns.kdeplot(rscs[2], label='K=10', ax=ax3)\n",
    "sns.kdeplot(rscs[3], label='K=25', ax=ax3)\n",
    "sns.kdeplot(rscs[4], label='K=100', ax=ax3)\n",
    "ax3.set_title('Distribution of Click Scores by K')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ks = [1,5,10,25,100]\n",
    "fig, axes = plt.subplots(5,3, figsize=(15,30))\n",
    "for i in range(5) :\n",
    "    ax1 = axes[i,0]\n",
    "    ax2 = axes[i,1]\n",
    "    ax3 = axes[i,2]\n",
    "    \n",
    "    sns.distplot(r2s[i], ax=ax1)\n",
    "    ax1.set_title('RPrec Scores | K = {}'.format(ks[i]))\n",
    "    ax1.set_xlabel('RPrec Score')\n",
    "    \n",
    "    sns.distplot(nds[i], ax=ax2)\n",
    "    ax2.set_title('NDCG Scores | K = {}'.format(ks[i]))\n",
    "    ax2.set_xlabel('NDCG Score')\n",
    "    \n",
    "    sns.distplot(rscs[i], ax=ax3)\n",
    "    ax3.set_title('Click Scores | K = {}'.format(ks[i]))\n",
    "    ax3.set_xlabel('Required Number of Clicks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and Interpretations\n",
    "Overall, we have seen that the filtering and network models perform the best, significantly improving over the baseline models using nearest neighbor techniques. Our final models were comparable with some of the top models in the RecSys challenge, so we are very satisfied with our results. If we had more time and computing power, we would have liked to scale both of those models up larger, as they were both limited in terms of their size (the network was trained on about 20000 playlists and ended up being about 7GB while filtering was only able to handle about **HOW MANY PLAYLISTS**). Ideally, we would be able to utilize sklearn's suppoer for sparse matrices to scale up filtering, but we weren't able to finalize that. \n",
    "\n",
    "Music recommendation in general is a challenging problem, with millions of songs to choose from and a large variety of songs within. More complex techniques like deep RNNs and autoencoders seemed attractive at the beginning of the project, but ultimately weren't feasible for us to complete. This forced us to adapt and implement the fairly different models seen here. Overall, we feel confident in our model's ability to find relevant songs to continue and put together a great playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "109_env",
   "language": "python",
   "name": "109_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
